{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Linear regression\n",
    "\n",
    "In this lab, you will review the details of linear regression. In particular:\n",
    "\n",
    "* How to formulate Matrices and solutions to Ordinary Least Squares (OLS).\n",
    "* `sns.lmplot` as a quick visual for Simple Linear Regression (SLR).\n",
    "* `scikit-learn`, or `sklearn` for short, a real-world data science tool that is more robust and flexible than analytical or `scipy.optimize` solutions. \n",
    "\n",
    "You will also practice interpreting residual plots (vs. fitted values) and the Multiple $R^2$ metric used in Multiple Linear Regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "np.random.seed(42)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For the first part of this lab, you will predict fuel efficiency (`mpg`) of several models of automobiles using a **single feature**: engine power (`horsepower`). For the second part, you will perform feature engineering on **multiple features** to better predict fuel efficiency.\n",
    "\n",
    "First, let's load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we load the fuel dataset, and drop any rows that have missing data.\n",
    "vehicle_data = sns.load_dataset('mpg').dropna()\n",
    "vehicle_data = vehicle_data.sort_values('horsepower', ascending=True)\n",
    "vehicle_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 392 datapoints and 8 potential features (plus our observed $y$ values, `mpg`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to fit a line to the plot below, which shows `mpg` vs. `horsepower` for several models of automobiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to visualize the data. \n",
    "sns.scatterplot(data=vehicle_data, x='horsepower', y='mpg');\n",
    "plt.title(\"mpg vs horsepower\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "\n",
    "### Question 1a: Construct $\\mathbb{X}$ with an intercept term\n",
    "\n",
    "\n",
    "Below, implement `add_intercept`, which creates a design matrix such that the first (left-most) column is all ones. The function has two lines: you are responsible for constructing the all-ones column `bias_feature` using the `np.ones` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.ones.html?highlight=ones)). This is then piped into a call to `np.concatenate` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html)), which we've implemented for you.\n",
    "\n",
    "**Note:** `bias_feature` should be a matrix of dimension `(n,1)`, not a vector of dimension `(n,)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    \"\"\"\n",
    "    Return X with a bias feature.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    X: a 2D DataFrame of p numeric features\n",
    "    (may also be a 2D NumPy array) of shape n x p\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A 2D matrix of shape n x (p + 1), where the leftmost\n",
    "    column is a column vector of 1's.\n",
    "    \"\"\"\n",
    "    bias_feature = ...\n",
    "    return np.concatenate([bias_feature, X], axis=1)\n",
    "\n",
    "# Note the [[ ]] brackets below: the argument needs to be\n",
    "# a matrix (DataFrame), as opposed to a single array (Series).\n",
    "X = add_intercept(vehicle_data[['horsepower']])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "### Question 1b: Define the OLS Model\n",
    "\n",
    "The predictions for all $n$ points in our data are:\n",
    "$$ \\Large \\hat{\\mathbb{Y}} = \\mathbb{X}\\theta $$\n",
    "where $\\theta = [\\theta_0, \\theta_1, \\dots, \\theta_p]$.\n",
    "\n",
    "Below, implement the `linear_model` function to evaluate this product.\n",
    "\n",
    "**Hint**: You can use `np.dot` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)), `pd.DataFrame.dot` ([documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dot.html)), or the `@` operator to multiply matrices/vectors. However, while the `@` operator can be used to multiply `NumPy` arrays, it generally will not work between two `pandas` objects, so keep that in mind when computing matrix-vector products!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_model(thetas, X):\n",
    "    \"\"\"\n",
    "    Return the linear combination of thetas and features as defined in the OLS equation.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    thetas: a 1D vector representing the parameters of our model ([theta0, theta1, ...]).\n",
    "    X: a 2D DataFrame of numeric features (may also be a 2D NumPy array).\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A 1D vector representing the linear combination of thetas and features as defined in the OLS equation.\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "\n",
    "### Question 1c: Least Squares Estimate, Analytically\n",
    "\n",
    "\n",
    "We showed in lecture that when $X^TX$ is invertible, the optimal estimate, $\\hat{\\theta}$, is given by the equation:\n",
    "\n",
    "$$ \\Large \\hat{\\theta} = (\\Bbb{X}^T\\Bbb{X})^{-1}\\Bbb{X}^T\\Bbb{Y}$$\n",
    "\n",
    "Below, implement the analytic solution to $\\hat{\\theta}$ using `np.linalg.inv` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html)) to compute the inverse of $\\Bbb{X}^T\\Bbb{X}$.\n",
    "\n",
    "**Hint 1**: To compute the transpose of a matrix, you can use `X.T` or `X.transpose()` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.T.html#numpy.ndarray.T)).\n",
    "\n",
    "**Note:** You can also consider using `np.linalg.solve` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html)) instead of `np.linalg.inv` because it is more robust (more on StackOverflow [here](https://stackoverflow.com/questions/31256252/why-does-numpy-linalg-solve-offer-more-precise-matrix-inversions-than-numpy-li)). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_analytical_sol(X, y):\n",
    "    \"\"\"\n",
    "    Computes the analytical solution to our\n",
    "    least squares problem\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    X: a 2D DataFrame (or NumPy array) of numeric features.\n",
    "    y: a 1D vector of outputs.\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    The estimate for theta (a 1D vector) computed using the\n",
    "    equation mentioned above.\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "Y = vehicle_data['mpg']\n",
    "analytical_thetas = get_analytical_sol(X, Y)\n",
    "analytical_thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Now, let's analyze our model's performance. Your task will be to interpret the model's performance using the two visualizations and one performance metric we've implemented below.\n",
    "\n",
    "First, we run `sns.lmplot`, which will both provide a scatterplot of `mpg` vs `horsepower` and display the least-squares line of best fit. (If you'd like to verify the OLS fit you found above is the same line found through `Seaborn`, change `include_OLS` to `True`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_OLS = False # Change this flag to visualize OLS fit\n",
    "\n",
    "sns.lmplot(data=vehicle_data, x='horsepower', y='mpg');\n",
    "predicted_mpg_hp_only = linear_model(analytical_thetas, X)\n",
    "if include_OLS:\n",
    "    # if flag is on, add OLS fit as a dotted red line\n",
    "    plt.plot(vehicle_data['horsepower'], predicted_mpg_hp_only, 'r--')\n",
    "plt.title(\"mpg vs horsepower\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we **plot the residuals.** While in Simple Linear Regression we have the option to plot residuals vs. the single input feature, in Multiple Linear Regression we often plot residuals vs. fitted values $\\hat{\\mathbb{Y}}$. In this lab, we opt for the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(predicted_mpg_hp_only, Y - predicted_mpg_hp_only)\n",
    "plt.axhline(0, c='black', linewidth=1)\n",
    "plt.xlabel(r'Fitted Values $\\hat{\\mathbb{Y}}$')\n",
    "plt.ylabel(r'Residuals $\\mathbb{Y} - \\hat{\\mathbb{Y}}$');\n",
    "plt.title(\"Residual plot\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, we compute the **correlation r** and **Multiple $R^2$** metric. As described in Lecture 12,\n",
    "\n",
    "$$R^2 = \\frac{\\text{variance of fitted values}}{\\text{variance of true } y} = \\frac{\\sigma_{\\hat{y}}^2}{\\sigma_y^2}$$\n",
    "\n",
    "$R^2$  can be used\n",
    "in the multiple regression setting, whereas $r$ (the correlation coefficient) is restricted to SLR since it depends on a single input feature.  In SLR, $r^{2}$ and Multiple $R^{2}$ are\n",
    "equivalent; the proof is left to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_hp_only = np.corrcoef(X[:, 1], Y)[0, 1]\n",
    "r2_hp_only = r_hp_only ** 2\n",
    "R2_hp_only = np.var(predicted_mpg_hp_only) / np.var(Y)\n",
    "\n",
    "print('Correlation, r, using only horsepower: ', r_hp_only)\n",
    "print('Correlation squared, r^2, using only horsepower: ', r2_hp_only)\n",
    "print('Multiple R^2 using only horsepower: ', r2_hp_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "### Question 1d\n",
    "\n",
    "In the cell below, comment on the above visualization and performance metrics, and whether `horsepower` and `mpg` have a good linear fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "## Question 2: Transform a Single Feature\n",
    "\n",
    "The Tukey-Mosteller Bulge Diagram (shown below) tells us to transform our $\\mathbb{X}$ or $\\mathbb{Y}$ to find a linear fit.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"tukey_mosteller.png\" width=\"300vw\" /></div>\n",
    "\n",
    "Let's consider the following linear model:\n",
    "\n",
    "$$\\text{predicted mpg} = \\theta_0 + \\theta_1 \\sqrt{\\text{horsepower}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "### Question 2a\n",
    "\n",
    "In the cell below, explain why we use the term \"linear\" to describe the model above, even though it incorporates a square root of horsepower  as a feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Introduction to `sklearn`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1. Create object.** \n",
    "\n",
    "We first create a `LinearRegression` object. Here's the `sklearn` [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). Note that by default, the object will include an intercept term when fitting.\n",
    "\n",
    "Here, `model` is like a \"blank slate\" for a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Run this cell to initialize a sklearn LinearRegression object.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# the `fit_intercept` argument controls whether or not the model should have an intercept (or bias) term\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2. `fit` the object to data.** \n",
    "\n",
    "Now, we need to tell `model` to \"fit\" itself to the data. Essentially, this is doing exactly what you did in the previous part of this lab (creating a risk function and finding the parameters that minimize that risk).\n",
    "\n",
    "**Note**: `X` needs to be a matrix (or `DataFrame`), as opposed to a single array (or `Series`) when running `model.fit`. This is because `sklearn.linear_model` is robust enough to be used for multiple regression, which we will look at later in this lab. This is why we use the double square brackets around `sqrt(hp)` when passing in the argument for `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Run this cell to add sqrt(hp) column for each car in the dataset.\n",
    "vehicle_data['sqrt(hp)'] = np.sqrt(vehicle_data['horsepower'])\n",
    "vehicle_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Run this cell to fit the model to the data.\n",
    "model.fit(X = vehicle_data[['sqrt(hp)']], y = vehicle_data['mpg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **3. Analyze fit.** \n",
    "\n",
    "Now that the model exists, we can look at the $\\hat{\\theta}_0$ and $\\hat{\\theta}_1$ values it found, which are given in the attributes `intercept` and `coef`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the `sklearn` linear regression model to make predictions, you can use the `model.predict` method.\n",
    "\n",
    "Below, we find the estimated `mpg` for a single datapoint with a `sqrt(hp)` of 6.78 (i.e., horsepower 46). Unlike the linear algebra approach, we do not need to manually add an intercept term because our `model` (which was created with `fit_intercept=True`) will automatically add one.\n",
    "\n",
    "**Note:** You may receive a user warning about missing feature names. This is due to the fact that we fitted on the feature DataFrame `vehicle_data[['sqrt(hp)']]` with feature names `\"sqrt(hp)\"` but only pass in a simple 2D arrays for prediction. To avoid this, we can convert our 2D array into a DataFrame with the matching feature name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs to be a 2D array since the X in step 2 was 2-dimensional.\n",
    "single_datapoint = [[6.78]]\n",
    "# Uncomment the following to see the result of predicting on a DataFrame instead of 2D array.\n",
    "#single_datapoint = pd.DataFrame([[6.78]], columns = ['sqrt(hp)']) # \n",
    "model.predict(single_datapoint) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2b\n",
    "\n",
    "Using the model defined above, which takes in `sqrt(hp)` as an input explanatory variable, predict the `mpg` for the full `vehicle_data` dataset. Assign the predictions to `predicted_mpg_hp_sqrt`. Running the cell will then compute the multiple $R^2$ value and create a linear regression plot for this new square root feature, overlaid on the original least squares estimate (used in Question 1c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_mpg_hp_sqrt = ...\n",
    "\n",
    "# Do not modify below this line.\n",
    "r2_hp_sqrt = np.var(predicted_mpg_hp_sqrt) / np.var(vehicle_data['mpg'])\n",
    "print('Multiple R^2 using sqrt(hp): ', r2_hp_sqrt)\n",
    "\n",
    "sns.lmplot(x = 'horsepower', y = 'mpg', data = vehicle_data)\n",
    "plt.plot(vehicle_data['horsepower'],  predicted_mpg_hp_sqrt,\n",
    "         color = 'r', linestyle='--', label='sqrt(hp) fit');\n",
    "plt.title(\"mpg vs. horsepower\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "The visualization shows a slight improvement, but the points on the scatter plot are still more \"curved\" than our prediction line. Let's try a quadratic feature instead! \n",
    "\n",
    "Next, we use the power of OLS to **add an additional feature.** Questions 1 and 2 utilized simple linear regression, a special case of OLS where we have 1 feature ($p=1$). For the following questions, we'll utilize multiple linear regression, which are cases of OLS when we have more than 1 features ($p > 1$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Add an Additional Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we move from SLR to multiple linear regression.\n",
    "\n",
    "Until now, we have established relationships between one independent explanatory variable and one response variable. However, with real-world problems, you will often want to use **multiple features** to model and predict a response variable. Multiple linear regression attempts to model the relationship between two or more explanatory variables and a response variable by fitting a linear equation to the observed data.\n",
    "\n",
    "We can consider including functions of existing features as **new features** to help improve the predictive power of our model. (This is something we will discuss in further detail in the Feature Engineering lecture.)\n",
    "\n",
    "The cell below adds a column that contains the square of the horsepower for each car in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to add a column of horsepower squared, no further action needed.\n",
    "vehicle_data['hp^2'] = vehicle_data['horsepower'] ** 2\n",
    "vehicle_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "## Question 3\n",
    "\n",
    "### Question 3a\n",
    "\n",
    "Using `sklearn`'s `LinearRegression`, create and fit a model that tries to predict `mpg` from `horsepower` AND `hp^2` using the DataFrame `vehicle_data`. Name your model `model_multi`.\n",
    "\n",
    "**Hint**: It should follow a similar format as Question 2.\n",
    "\n",
    "**Note**: You must create a new model again using `LinearRegression()`, otherwise the old model from Question 2 will be overwritten. If you do overwrite it,just restart your kernel and run your cells in order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_multi = LinearRegression() # By default, fit_intercept=True\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting, we can see the coefficients and intercept. Note that there are now two elements in `model_multi.coef_`, since there are two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "\n",
    "### Question 3b\n",
    "\n",
    "Using the above values, write out the function that the model is using to predict `mpg` from `horsepower` and `hp^2`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "The plot below shows the prediction of our model. It's much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to show the prediction of our model.\n",
    "predicted_mpg_multi = model_multi.predict(vehicle_data[['horsepower', 'hp^2']])\n",
    "r2_multi = np.var(predicted_mpg_multi) / np.var(vehicle_data['mpg'])\n",
    "print('Multiple R^2 using both horsepower and horsepower squared: ', r2_multi)\n",
    "\n",
    "sns.scatterplot(x = 'horsepower', y = 'mpg', data = vehicle_data)\n",
    "plt.plot(vehicle_data['horsepower'],  predicted_mpg_hp_only, label='hp only');\n",
    "plt.plot(vehicle_data['horsepower'],  predicted_mpg_hp_sqrt, color = 'r', linestyle='--', label='sqrt(hp) fit');\n",
    "plt.plot(vehicle_data['horsepower'],  predicted_mpg_multi, color = 'gold', linewidth=2, label='hp and hp^2');\n",
    "plt.title(\"mpg vs horsepower\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By incorporating a squared feature, we are able to capture the curvature of the dataset. Our model is now a parabola centered on our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3c\n",
    "\n",
    "In the cell below, we assign the mean of the `mpg` column of the `vehicle_data` DataFrame to `mean_mpg`. Given this information, what is the mean of the `mean_predicted_mpg_hp_only`, `predicted_mpg_hp_sqrt`, and `predicted_mpg_multi` arrays?\n",
    "\n",
    "**Hint**: Your answer should be a function of `mean_mpg` provided, you should not have to call `np.mean` in your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_mpg = np.mean(vehicle_data['mpg'])\n",
    "mean_predicted_mpg_hp_only = ...\n",
    "mean_predicted_mpg_hp_sqrt = ...\n",
    "mean_predicted_mpg_multi = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing this model with previous models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compares q1, q2, q3, and overfit models (ignores redundant model)\n",
    "print('Multiple R^2 using only horsepower: ', r2_hp_only)\n",
    "print('Multiple R^2 using sqrt(hp): ', r2_hp_sqrt)\n",
    "print('Multiple R^2 using both hp and hp^2: ', r2_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the R^2 value of the last model is the highest. In fact, it can be proven that multiple R^2 will not decrease as we add more variables. You may be wondering, what will happen if we add more variables? We will discuss the limitations of adding too many variables in an upcoming lecture. Below, we consider an extreme case that we include a variable twice in the model.\n",
    "\n",
    "You might also be wondering why we chose to use `hp^2` as our additional feature, even though that transformation in the Tukey-Mosteller Bulge Diagram doesn't correspond to the bulge in our data. The Bulge diagram is a good starting point for transforming our data, but you may need to play around with different transformations to see which of them is able to capture the true relationship in our data and create a model with the best fit. This trial and error process is a very useful technique used all throughout data science!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Faulty Feature Engineering: Redundant Features\n",
    "\n",
    "Suppose we used the following linear model:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{mpg} &= \\theta_0 + \\theta_1 \\cdot \\text{horsepower} + \\theta_2 \\cdot \\text{horsepower}^2 + \\theta_3 \\cdot \\text{horsepower}\n",
    "\\end{align}\n",
    "\n",
    "Notice that `horsepower` appears twice in our model!! We will explore how this redundant feature affects our modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 4\n",
    "\n",
    "### Question 4a: Linear Algebra\n",
    "\n",
    "Construct a matrix `X_redundant` that uses the `vehicle_data` DataFrame to encode the \"three\" features above, as well as a bias feature.\n",
    "\n",
    "**Hint**: Use the `add_intercept` term you implemented in Question 1a.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_redundant = ...\n",
    "X_redundant.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now, run the cell below to find the analytical OLS Estimate using the `get_analytical_sol` function you wrote in Question 1c.\n",
    "\n",
    "**Note:** Depending on the machine that you run your code on, you should either **see a singular matrix error** or **end up with thetas that are nonsensical** (magnitudes greater than $10^{15}$). In other words, if the cell below errors, that is by design, it is supposed to error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check the result, no further action needed. \n",
    "# The try-except block suppresses errors during submission\n",
    "import traceback\n",
    "try:\n",
    "    analytical_thetas = get_analytical_sol(X_redundant, vehicle_data['mpg'])\n",
    "    analytical_thetas\n",
    "except Exception as e:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4b\n",
    "\n",
    "In the cell below, explain why we got the error above when trying to calculate the analytical solution to predict `mpg`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "Note: While we encountered errors when using the linear algebra approach, a model fitted with `sklearn` will not encounter matrix singularity errors since it uses numerical methods to find optimums ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn finds optimal parameters despite redundant features\n",
    "model_redundant = LinearRegression(fit_intercept=False) # X_redundant already has an intercept column\n",
    "model_redundant.fit(X = X_redundant, y = vehicle_data['mpg'])\n",
    "model_redundant.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Feature Engineering\n",
    "\n",
    "To begin, let's load the tips dataset from the `seaborn` library.  This dataset contains records of tips, total bill, and information about the person who paid the bill. As earlier, we'll be trying to predict tips from the other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the tips dataset; no further action is needed.\n",
    "data = sns.load_dataset(\"tips\")\n",
    "\n",
    "print(\"Number of Records:\", len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Defining the Model and Engineering Features\n",
    "\n",
    " Now, let's make a more complicated model that utilizes other features in our dataset. You can imagine that we might want to use the features with an equation that looks as shown below:\n",
    "\n",
    "$$ \\text{Tip} = \\theta_0 + \\theta_1 \\cdot \\text{total}\\_\\text{bill} + \\theta_2 \\cdot \\text{sex} + \\theta_3 \\cdot \\text{smoker} + \\theta_4 \\cdot \\text{day} + \\theta_5 \\cdot \\text{time} + \\theta_6 \\cdot \\text{size} $$\n",
    "\n",
    "Unfortunately, that's not possible because some of these features like \"day\" are not numbers, so it doesn't make sense to multiply by a numerical parameter. Let's start by converting some of these non-numerical values into numerical values.\n",
    "\n",
    "Before we do this, let's separate out the tips and the features into two separate variables, and add a bias term using `pd.insert` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.insert.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to create our design matrix X; no further action is needed.\n",
    "tips = data['tip']\n",
    "X = data.drop(columns='tip')\n",
    "X.insert(0, 'bias', 1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Question 5: Feature Engineering\n",
    "\n",
    "Let's use **one-hot encoding** to better represent the days! For example, we encode Sunday as the row vector `[0 0 0 1]` because our dataset only contains bills from Thursday through Sunday. This replaces the `day` feature with four boolean features indicating if the record occurred on Thursday, Friday, Saturday, or Sunday. One-hot encoding therefore assigns a more even weight across each category in non-numeric features.\n",
    "\n",
    "Complete the code below to one-hot encode our dataset. This `DataFrame` holds our \"featurized\" data, which is also often denoted by $\\phi$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(data):\n",
    "    \"\"\"\n",
    "    Return the one-hot encoded DataFrame of our input data.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data: A DataFrame that may include non-numerical features.\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A one-hot encoded DataFrame that only contains numeric features.\n",
    "    \n",
    "    \"\"\"\n",
    "    ...\n",
    "    return data\n",
    "\n",
    "    \n",
    "one_hot_X = one_hot_encode(X)\n",
    "one_hot_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Tutorial: fit()/predict()\n",
    "\n",
    "Now that all of our data is numeric, we can begin to define our model function. Notice that after one-hot encoding our data, we now have 13 features instead of 7 (including bias). Therefore, our linear model is now similar to the below (note the order of thetas below does not necessarily match the order in the `DataFrame`):\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Tip} &= \\theta_0 + \\theta_1 \\cdot \\text{total}\\_\\text{bill} + \\theta_2 \\cdot \\text{size}  \\\\\n",
    "& + \\theta_3 \\cdot \\text{sex}\\_\\text{Female} + \\theta_4 \\cdot \\text{sex}\\_\\text{Male} \\\\\n",
    "& + \\theta_5 \\cdot \\text{smoker}\\_\\text{No} + \\theta_{6} \\cdot \\text{smoker}\\_\\text{Yes} \\\\\n",
    "& + \\theta_7 \\cdot \\text{day}\\_\\text{Fri} + \\theta_8 \\cdot \\text{day}\\_\\text{Sat} + \\theta_9 \\cdot \\text{day}\\_\\text{Sun} + \\theta_{10} \\cdot \\text{day}\\_\\text{Thur} \\\\\n",
    "& + \\theta_{11} \\cdot \\text{time}\\_\\text{Dinner}+ \\theta_{12} \\cdot \\text{time}\\_\\text{Lunch} \n",
    "\\end{align}\n",
    "\n",
    "We can represent the linear combination above as a matrix-vector product. To practice using syntax similar to the `sklearn` pipeline, we introduce a toy example called `MyZeroLinearModel`.\n",
    "\n",
    "The `MyZeroLinearModel` has two methods, `predict` and `fit`.\n",
    "* `fit`: Compute parameters theta given data `X` and `Y` and the underlying model.\n",
    "* `predict`: Compute estimate $\\hat{y}$ given `X` and the underlying model.\n",
    "\n",
    "If you are unfamiliar with using `Python` objects, please review object-oriented programming. \n",
    "\n",
    "**Note:** Practically speaking, this is a pretty bad model: it sets all of its parameters to 0 regardless of the data we fit it to! While this model doesn't really have any practical application, we're using it here to help you build intuition on how `sklearn` pipelines work! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to create the MyZeroLinearModel class; no further action is needed.\n",
    "class MyZeroLinearModel():\n",
    "    def __init__(self):\n",
    "        self._thetas = None\n",
    "    def fit(self, X, Y):\n",
    "        number_of_features = X.shape[1]\n",
    "        # For demonstration purposes in this tutorial, we set the values of all the parameters to 0. \n",
    "        self._thetas = np.zeros(shape=(number_of_features, 1))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self._thetas\n",
    "\n",
    "# Running the code below produces all-zero thetas\n",
    "model0 = MyZeroLinearModel()\n",
    "model0.fit(one_hot_X, tips)\n",
    "model0._thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Question 6: Fitting a Linear Model Using Numerical Methods\n",
    "\n",
    "\n",
    "The best-fit model is determined by our loss function.  We can define multiple loss functions and found the optimal $\\hat{\\theta}$ using the `scipy.optimize.minimize` function. \n",
    "\n",
    "\n",
    "In this question, we'll wrap this function into a method `fit()` in our class `MyScipyLinearModel`.\n",
    "To allow for different loss functions, we create a `loss_function` parameter where the model can be fit accordingly. Example loss functions are given as `l1` and `l2`.\n",
    "\n",
    "**Note:** Just like `MyZeroLinearModel`, the class `MyScipyLinearModel` is a toy example to help you understand how `sklearn` works behind the scenes. In practice, when using pre-made `sklearn` models, defining a class like this is unnecessary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 6a: scipy\n",
    "\n",
    "Complete the code below using `scipy.optimize.minimize`. Find and store the optimal $\\hat{\\theta}$ in the instance attribute `self._thetas`.\n",
    "\n",
    "**Hint:**\n",
    "* The `starting_guess` should be some arbitrary array (such as an array of all zeroes) of the correct length. You may find `number_of_features` helpful.\n",
    "\n",
    "**Notes:**\n",
    "* Notice that `l1` and `l2` return term-wise loss and only accept observed value $y$ and predicted value $\\hat{y}$. We added a lambda function to help convert them into the right format for `scipy.optimize.minimize`.\n",
    "* Notice above that we extract the `'x'` entry in the dictionary returned by `minimize`. This entry corresponds to the optimal $\\hat{\\theta}$ estimated by the function, and it is the format that `minimize` uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def l1(y, y_hat):\n",
    "    return np.abs(y - y_hat)\n",
    "\n",
    "def l2(y, y_hat):\n",
    "    return (y - y_hat)**2\n",
    "\n",
    "class MyScipyLinearModel():\n",
    "    def __init__(self):\n",
    "        self._thetas = None\n",
    "        \n",
    "    def fit(self, loss_function, X, Y):\n",
    "        \"\"\"\n",
    "        Estimated optimal _thetas for the given loss function, \n",
    "        feature matrix X, and observed values y. Store them in _thetas.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        loss_function: A function that takes in observed and predicted y, \n",
    "                       and return the loss calculated for each data point.\n",
    "        X: A 2D DataFrame (or NumPy array) of numeric features.\n",
    "        Y: A 1D NumPy array or Series of the dependent variable.\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        number_of_features = X.shape[1]\n",
    "        starting_guess = ...\n",
    "        self._thetas = minimize(lambda theta:\n",
    "                                ...\n",
    "                                , x0 = starting_guess)['x']        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        return X @ self._thetas\n",
    "        \n",
    "# Create a new model and fit the data using l2 loss, it should produce some non-zero thetas.\n",
    "model = MyScipyLinearModel()\n",
    "model.fit(l2, one_hot_X, tips)\n",
    "print(\"L2 loss thetas:\")\n",
    "print(model._thetas)\n",
    "\n",
    "# Create a new model and fit the data using l1 loss, it should produce some non-zero thetas.\n",
    "model_l1 = MyScipyLinearModel()\n",
    "model_l1.fit(l1, one_hot_X, tips)\n",
    "print(\"L1 loss thetas:\")\n",
    "print(model._thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE and MAE for your model above should be just slightly larger than 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to calculate the MSE and MAE of the above model; no further action is needed.\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"L2 loss MSE scipy: \" + str(mean_squared_error(model.predict(one_hot_X), tips)))\n",
    "print(\"L1 loss MAE scipy: \" + str(mean_squared_error(model_l1.predict(one_hot_X), tips)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Question 6b: sklearn\n",
    "\n",
    "Another way to fit a linear regression model is to use `scikit-learn`/`sklearn` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "sklearn_model = ...\n",
    "...\n",
    "print(\"sklearn with bias column thetas:\")\n",
    "print(sklearn_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    " \n",
    "---\n",
    " \n",
    "### Question 6c: sklearn and `fit_intercept`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid always explicitly building in a bias column into our design matrix, `sklearn`'s `LinearRegression` object also supports `fit_intercept=True` during instantiation. \n",
    "\n",
    "Fill in the code below by first assigning `one_hot_X_nobias` to the `one_hot_X` design matrix with the bias column dropped, then fit a new `LinearRegression` model, with intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_X_nobias = ...\n",
    "\n",
    "sklearn_model_intercept = ...\n",
    "...\n",
    "\n",
    "# Note that sklearn returns intercept (theta_0) and coefficients (other thetas) separately.\n",
    "# We concatenate the intercept and other thetas before printing for easier comparison with the models above.\n",
    "print(\"sklearn with intercept thetas:\")\n",
    "print(np.concatenate(([sklearn_model_intercept.intercept_], sklearn_model_intercept.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Question 7: Fitting the Model Using Analytic Methods\n",
    "\n",
    "Let's also fit our model analytically for the L2 loss function. Recall from lecture that with a linear model, we are solving the following optimization problem for least squares:\n",
    "\n",
    "$$\\min_{\\theta} \\frac{1}{n}||\\Bbb{Y} - \\Bbb{X}\\theta||^2$$\n",
    "\n",
    "We showed in lecture that the optimal $\\hat{\\theta}$ when $\\Bbb{X}^T\\Bbb{X}$ is invertible is given by the equation: $(\\Bbb{X}^T\\Bbb{X})^{-1}\\Bbb{X}^T\\Bbb{Y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 7a: Analytic Solution Using Explicit Inverses\n",
    "\n",
    "For this problem, implement the analytic solution above using `np.linalg.inv` to compute the inverse of $\\Bbb{X}^T\\Bbb{X}$. We provide a class `MyAnalyticallyFitOLSModel` with a `fit` method to wrap this functionality.\n",
    "\n",
    "**Hint**: To compute the transpose of a matrix, you can use `X.T` or `X.transpose()`.\n",
    "\n",
    "**Note**: We want our thetas to always be a `NumPy array` object, even if `Y` is a `Series`. If you are using the `@` `NumPy` operator, make sure you are correctly placing parentheses around expressions where needed to make this happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAnalyticallyFitOLSModel():\n",
    "    def __init__(self):\n",
    "        self._thetas = None\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Sets _thetas using the analytical solution to the OLS problem.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X: A 2D DataFrame (or NumPy array) of numeric features (one-hot encoded).\n",
    "        Y: A 1D NumPy array or Series of the dependent variable.\n",
    "        \n",
    "        Returns\n",
    "        -----------\n",
    "        None\n",
    "        \"\"\"\n",
    "        ...\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self._thetas        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to find the analytical solution for the `tips` dataset. Depending on the machine that you run your code on, **you should either see a singular matrix error or end up with some theta values that are nonsensical (magnitudes greater than $10^{15}$).** This is not good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check the result, no further action needed.\n",
    "# The try-except block suppresses errors during submission\n",
    "import traceback\n",
    "try:\n",
    "    model_analytical = MyAnalyticallyFitOLSModel()\n",
    "    model_analytical.fit(one_hot_X, tips)\n",
    "    analytical_thetas = model_analytical._thetas\n",
    "    print(analytical_thetas)\n",
    "except Exception as e:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, explain why we got the error or nonsensical theta values above when trying to calculate the analytical solution for our one-hot encoded `tips` dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "### Question 7c: Fixing Our One-Hot Encoding\n",
    "\n",
    "Now, let's modify our one-hot encoding approach from earlier so we don't get the error we saw in the previous part. Complete the code below to one-hot-encode our dataset such that `one_hot_X_revised` has no redundant features. \n",
    "\n",
    "**Hint**: To identify redundancies in one-hot-encoded features, consider the number of boolean values that are required to uniquely express each possible option. For example, we only need one column to express whether an individual it's Lunch or Dinner time: If the value is 0 in the Lunch column, it tells us it must be Dinner time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_revised(data):\n",
    "    \"\"\"\n",
    "    Return the one-hot encoded DataFrame of our input data, removing redundancies.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data: A DataFrame that may include non-numerical features.\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A one-hot encoded DataFrame that only contains numeric features without any redundancies.\n",
    "    \n",
    "    \"\"\"\n",
    "    ...\n",
    "    return data\n",
    "\n",
    "one_hot_X_revised = one_hot_encode_revised(X)\n",
    "display(one_hot_X_revised.head())\n",
    "    \n",
    "scipy_model = MyScipyLinearModel()\n",
    "scipy_model.fit(l2, one_hot_X_revised, tips)\n",
    "    \n",
    "analytical_model = MyAnalyticallyFitOLSModel()\n",
    "analytical_model.fit(one_hot_X_revised, tips)\n",
    "\n",
    "print(\"Our scipy numerical model's loss is: \", mean_squared_error(scipy_model.predict(one_hot_X_revised), tips))\n",
    "print(\"Our analytical model's loss is: \", mean_squared_error(analytical_model.predict(one_hot_X_revised), tips))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the rank of the matrix using the `NumPy` function `np.linalg.matrix_rank`. We have printed the rank of the data and number of columns for you below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"one_hot_X: \\n\"\n",
    "      + \"\\t number of columns: \" + str(len(one_hot_X.columns)) \\\n",
    "      + \"\\trank: \" + str(np.linalg.matrix_rank(one_hot_X)))\n",
    "print(\"one_hot_X_revised: \\n\"\n",
    "      + \"\\t number of columns: \" + str(len(one_hot_X_revised.columns)) \\\n",
    "      + \"\\trank: \" + str(np.linalg.matrix_rank(one_hot_X_revised)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X.shape == (392, 2)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (add_intercept(np.array([[1, 2, 3], [4, 5, 6]]).T)[:, 0] == np.ones((3,))).all()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> add_intercept(np.array([[1, 2, 3], [4, 5, 6]]).T).shape == (3, 3)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (add_intercept(np.array([[1, 2, 3], [4, 5, 6]]).T)[:, 2] == np.array([4, 5, 6])).all()\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> linear_model(np.arange(1, 5), np.arange(1, 5)) == 30\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (linear_model(2 * np.eye(100), np.ones(100)) == 2 * np.ones(100)).all()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> test_theta = np.array([[1, 2], [3, 4], [5, 6]])\n>>> test_x = np.array([[1, 3, 5], [2, 4, 6]])\n>>> expected = np.array([[35, 44], [44, 56]])\n>>> actual = linear_model(test_theta, test_x)\n>>> np.array_equal(actual, expected)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> test_theta_2 = np.array([[3], [5]])\n>>> test_x_2 = np.array([[1, 4], [1, 6], [1, 8]])\n>>> expected_2 = np.array([[23], [33], [43]])\n>>> actual_2 = linear_model(test_theta_2, test_x_2)\n>>> np.array_equal(expected_2, actual_2)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> analytical_thetas.shape in ((2,), (2, 1))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(analytical_thetas[0], 39.93586102)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(model_multi.intercept_, 56.90009970211295)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(model_multi.coef_[0], -0.46618963)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(model_multi.coef_[1], 0.00123054)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(model.intercept_, 58.70517203721748) and np.isclose(model.coef_[0], -3.50352375)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3c": {
     "name": "q3c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(mean_predicted_mpg_hp_only, np.mean(predicted_mpg_hp_only), atol=0.001)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(mean_predicted_mpg_hp_sqrt, np.mean(predicted_mpg_hp_sqrt), atol=0.001)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(mean_predicted_mpg_multi, np.mean(predicted_mpg_multi), atol=0.001)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_redundant.shape == (392, 4)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
