{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e0f9b2de18190d9d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Project: Cook County Housing Price Prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project explores what can be learned from an extensive housing dataset that is embedded in a dense social context in Cook County, Illinois.\n",
    "\n",
    "First, we will guide you through some basic Exploratory Data Analysis (EDA) to understand the structure of the data. Next, you will be adding a few new features to the dataset, while cleaning the data as well in the process.\n",
    "\n",
    "Next, you will specify and fit a linear model for the purpose of prediction. Finally, we will analyze the error of the model and brainstorm ways to improve the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-62cfd21463535cac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# The Data\n",
    "\n",
    "The dataset consists of over 500,000 records from Cook County, Illinois, the county where Chicago is located. The dataset has 61 features in total; the 62nd is `Sale Price`, which you will predict with regression. An explanation of each variable can be found in the included `codebook.txt` file (you can optionally open this by first clicking the `data` folder, then clicking `codebook.txt` file in the navigation pane). Some of the columns have been filtered out to ensure this assignment doesn't become overly long when dealing with data cleaning and formatting.\n",
    "\n",
    "The data are split into training and test sets with 204,792 and 68,264 observations, respectively, but we will only be working on the training set for EDA\n",
    "\n",
    "Let's first extract the data from the `cook_county_data.zip`. Notice we didn't leave the `csv` files directly in the directory because they take up too much space without some prior compression. Just run the cells below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('data/cook_county_data.zip') as item:\n",
    "    item.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the initial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8fea30adc9d489b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "initial_data = pd.read_csv(\"cook_county_train.csv\",index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9d6d509b6e854e10",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "As a good sanity check, we should at least verify that the data shape matches the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c841a2de55691502",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# 204,792 observations and 62 features in training data\n",
    "assert initial_data.shape == (204792, 62)\n",
    "# Sale Price is provided in the training data\n",
    "assert 'Sale Price' in initial_data.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce9acc2f62c96e59",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The next order of business is getting a feel for the variables in our data.  A more detailed description of each variable is included in `codebook.txt` (in the same directory as this notebook).  **You should take some time to familiarize yourself with the codebook before moving forward.**\n",
    "\n",
    "Let's take a quick look at all the current columns in our initial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4e60a7a0cda5eecf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "initial_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data['Description'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 1: Contextualizing the Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 1a\n",
    "\n",
    "Based on the columns in this dataset and the values that they take, what do you think each row represents? That is, what is the granularity of this dataset? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 1b\n",
    "Why do you think this data was collected? For what purposes? By whom?\n",
    "\n",
    "This question calls for your speculation and is looking for thoughtfulness, not correctness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 1c\n",
    "\n",
    "Craft at least two questions about housing in Cook County that can be answered with this dataset and provide the type of analytical tool you would use to answer it (e.g. \"I would create a ___ plot of ___ and ___\" or \"I would calculate the ___ [summary statistic] for ___ and ____\"). Be sure to reference the columns that you would use and any additional datasets you would need to answer that question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 1d\n",
    "\n",
    "Suppose now, in addition to the information already contained in the dataset, you also have access to several new columns containing demographic data about the owner, including race/ethnicity, gender, age, annual income, and occupation. Provide one new question about housing in Cook County that can be answered using at least one column of demographic data and at least one column of existing data and provide the type of analytical tool you would use to answer it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba0f6926b0dafefb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/><br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 2: Exploratory Data Analysis\n",
    "\n",
    "This dataset was collected by the [Cook County Assessor's Office](https://datacatalog.cookcountyil.gov/Property-Taxation/Archive-Cook-County-Assessor-s-Residential-Sales-D/5pge-nu6u) in order to build a model to predict the monetary value of a home. In this section, we will make a series of exploratory visualizations and feature engineering in preparation for that prediction task.\n",
    "\n",
    "Note that we will perform EDA on the **initial data**.\n",
    "\n",
    "### Sale Price\n",
    "We begin by examining the distribution of our target variable `Sale Price`. We have provided the following helper method `plot_distribution` that you can use to visualize the distribution of the `Sale Price` using both the histogram and the box plot at the same time. Run the following 2 cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15d483a695655cea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_distribution(data, label):\n",
    "    fig, axs = plt.subplots(nrows=2)\n",
    "\n",
    "    sns.distplot(\n",
    "        data[label], \n",
    "        ax=axs[0]\n",
    "    )\n",
    "    sns.boxplot(\n",
    "        x=data[label],\n",
    "        width=0.3, \n",
    "        ax=axs[1],\n",
    "        showfliers=False,\n",
    "    )\n",
    "\n",
    "    # Align axes\n",
    "    spacer = np.max(data[label]) * 0.05\n",
    "    xmin = np.min(data[label]) - spacer\n",
    "    xmax = np.max(data[label]) + spacer\n",
    "    axs[0].set_xlim((xmin, xmax))\n",
    "    axs[1].set_xlim((xmin, xmax))\n",
    "\n",
    "    # Remove some axis text\n",
    "    axs[0].xaxis.set_visible(False)\n",
    "    axs[0].yaxis.set_visible(False)\n",
    "    axs[1].yaxis.set_visible(False)\n",
    "\n",
    "    # Put the two plots together\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "    fig.suptitle(\"Distribution of \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(initial_data, label='Sale Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the same time, we also take a look at some descriptive statistics of this variable. Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data['Sale Price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 2a\n",
    "\n",
    "Using the plots and the descriptive statistics from `initial_data['Sale Price'].describe()` above, identify one issue with the visualization above and briefly describe one way to overcome it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional cell for scratch work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 2b\n",
    "\n",
    "To zoom in on the visualization of most households, we will focus only on a subset of `Sale Price` for this assignment. In addition, it may be a good idea to apply a log transformation to `Sale Price`. In the cell below, assign `training_data` to a new `DataFrame` that is the same as `initial_data` **except with the following changes**:\n",
    "\n",
    "- `training_data` should contain only households whose price is at least $500.\n",
    "- `training_data` should contain a new `Log Sale Price` column that contains the log-transformed sale prices.\n",
    "\n",
    "**You should NOT remove or modify the original column `Sale Price` as it will be helpful for later questions.** If you accidentally remove it, just restart your kernel and run the cells again.\n",
    "\n",
    "**Note**: This also implies from now on, our target variable in the model will be the log-transformed sale prices from the column `Log Sale Price`. \n",
    "\n",
    "*To ensure that any error from this part does not propagate to later questions, there will be no hidden tests for this question.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new distribution plot using the log-transformed sale prices. As a sanity check, you should see that the distribution for `Log Scale Price` is much more uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(training_data, label='Log Sale Price');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 3a\n",
    "\n",
    "\n",
    "Is the following statement correct? Assign your answer to `q3statement`.\n",
    "\n",
    "     \"At least 25% of the properties in the training set sold for more than $200,000.00.\"\n",
    "\n",
    "**Note:** The provided test for this question does not confirm that you have answered correctly; only that you have assigned `q3statement` to `True` or `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1-answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This should be set to True or False\n",
    "q3statement = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 3b\n",
    "\n",
    "Next, we want to explore if there is any correlation between `Log Sale Price` and the total area occupied by the property. The `codebook.txt` file tells us the column `Building Square Feet` should do the trick — it measures \"(from the exterior) the total area, in square feet, occupied by the building\".\n",
    "\n",
    "Let's also apply a log transformation to the `Building Square Feet` column.\n",
    "\n",
    "In the following cell, create a new column `Log Building Square Feet` in our `training_data` that contains the log-transformed area occupied by each property. \n",
    "\n",
    "**You should NOT remove or modify the original `Building Square Feet` column as it will be used for later questions**. If you accidentally remove it, just restart your kernel and run the cells again.\n",
    "\n",
    "*To ensure that any errors from this part do not propagate to later questions, there will be no hidden tests for this question.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data['Log Building Square Feet'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 3c\n",
    "\n",
    "In the visualization below, we created a `jointplot` with `Log Building Square Feet` on the x-axis, and `Log Sale Price` on the y-axis. In addition, we fit a simple linear regression line through the bivariate scatter plot in the middle.\n",
    "\n",
    "Based on the following plot, would `Log Building Square Feet` make a good candidate as one of the features for our model? Why or why not?\n",
    "\n",
    "**Hint:** To help answer this question, ask yourself: what kind of relationship does a “good” feature share with the target variable we aim to predict?\n",
    "\n",
    "![Joint Plot](images/q2p3_jointplot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 4\n",
    "\n",
    "Continuing from the previous part, as you explore the dataset, you might still run into more outliers that prevent you from creating a clear visualization or capturing the trend of the majority of the houses. \n",
    "\n",
    "Write a function `remove_outliers` that removes outliers from the dataset based on a threshold value of a variable. For example, `remove_outliers(training_data, 'Building Square Feet', lower=500, upper=8000)` should return a copy of `data` with only observations that satisfy `Building Square Feet` less than or equal to 8000 (inclusive) and `Building Square Feet` greater than 500 (inclusive).\n",
    "\n",
    "**Note:** The provided tests simply check that the `remove_outliers` function you defined does not mutate the input data in-place. They do not check that you have implemented `remove_outliers` correctly so that it works with any data, variable, lower, and upper bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9186ec2ca053d0aa",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_outliers(data, variable, lower=-np.inf, upper=np.inf):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (DataFrame): the table to be filtered\n",
    "      variable (string): the column with numerical outliers\n",
    "      lower (numeric): observations with values lower than or equal to this will be removed\n",
    "      upper (numeric): observations with values higher than or equal to this will be removed\n",
    "    \n",
    "    Output:\n",
    "      a DataFrame with outliers removed\n",
    "      \n",
    "    Note: This function should not change mutate the contents of data.\n",
    "    \"\"\"  \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 3: Feature Engineering\n",
    "\n",
    "In this section, we will walk you through a few feature engineering techniques. \n",
    "\n",
    "### Bedrooms\n",
    "\n",
    "Let's start simple by extracting the total number of bedrooms as our first feature for the model. You may notice that the `Bedrooms` column doesn't actually exist in the original `DataFrame`! Instead, it is part of the `Description` column.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 5a\n",
    "\n",
    "Let's take a closer look at the `Description` column first. Compare the description for a few rows. For the following list of variables, how many of them can be extracted from the `Description` column? Assign your answer to a list of integers corresponding to the statements that you think are true (ie. `[1, 2, 3]`).\n",
    "\n",
    "1. The date the property was sold on.\n",
    "2. The number of stories the property contains.\n",
    "3. The previous owner of the property.\n",
    "4. The address of the property.\n",
    "5. The number of garages the property has.\n",
    "6. The total number of rooms inside the property.\n",
    "7. The total number of bedrooms inside the property.\n",
    "8. The total number of bathrooms inside the property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q5a = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 5b\n",
    "\n",
    "Write a function `add_total_bedrooms(data)` that returns a copy of `data` with an additional column called `Bedrooms` that contains the total number of bedrooms (**as integers**) for each house. Treat missing values as zeros, if necessary. Remember that you can make use of vectorized code here; you shouldn't need any `for` statements. \n",
    "\n",
    "**Hint**: You should consider inspecting the `Description` column to figure out if there is any general structure within the text. Once you have noticed a certain pattern, you are set with the power of RegEx!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_total_bedrooms(data):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (DataFrame): a DataFrame containing at least the Description column.\n",
    "\n",
    "    Output:\n",
    "      a Dataframe with a new column \"Bedrooms\" containing ints.\n",
    "\n",
    "    \"\"\"\n",
    "    with_rooms = data.copy()\n",
    "    ...\n",
    "    return with_rooms\n",
    "\n",
    "training_data = add_total_bedrooms(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 5c\n",
    "\n",
    "Create a visualization that clearly and succinctly shows if there exists an association between  `Bedrooms` and `Log Sale Price`. A good visualization should satisfy the following requirements:\n",
    "- It should avoid overplotting.\n",
    "- It should have clearly labeled axes and a succinct title.\n",
    "- It should convey the strength of the correlation between `Sale Price` and the number of rooms: in other words, you should be able to look at the plot and describe the general relationship between `Log Sale Price` and `Bedrooms`.\n",
    "\n",
    "**Hint**: A direct scatter plot of the `Sale Price` against the number of rooms for all of the households in our training data might risk overplotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "Now, let's take a look at the relationship between neighborhood and sale prices of the houses in our dataset.\n",
    "Notice that currently we don't have the actual names for the neighborhoods. Instead we will use a similar column, `Neighborhood Code` (which is a numerical encoding of the actual neighborhoods by the Assessment office)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 6a\n",
    "\n",
    "Before creating any visualization, let's quickly inspect how many different neighborhoods we are dealing with.\n",
    "\n",
    "Assign the variable `num_neighborhoods` to the total number of **unique** neighborhoods in `training_data`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_neighborhoods = ...\n",
    "num_neighborhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 6b\n",
    "\n",
    "If we try directly plotting the distribution of `Log Sale Price` for all of the households in each neighborhood using the `plot_categorical` function from the next cell, we get the following visualization.\n",
    "\n",
    "\n",
    "![overplot](images/q5p2_catplot.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feel free to create a cell below this and run plot_cateogrical(training_data) if you want to see what this function outputs.\n",
    "def plot_categorical(neighborhoods):\n",
    "    fig, axs = plt.subplots(nrows=2)\n",
    "\n",
    "    sns.boxplot(\n",
    "        x='Neighborhood Code',\n",
    "        y='Log Sale Price',\n",
    "        data=neighborhoods,\n",
    "        ax=axs[0],\n",
    "    )\n",
    "\n",
    "    sns.countplot(\n",
    "        x='Neighborhood Code',\n",
    "        data=neighborhoods,\n",
    "        ax=axs[1],\n",
    "    )\n",
    "\n",
    "    # Draw median price\n",
    "    axs[0].axhline(\n",
    "        y=training_data['Log Sale Price'].median(), \n",
    "        color='red',\n",
    "        linestyle='dotted'\n",
    "    )\n",
    "\n",
    "    # Label the bars with counts\n",
    "    for patch in axs[1].patches:\n",
    "        x = patch.get_bbox().get_points()[:, 0]\n",
    "        y = patch.get_bbox().get_points()[1, 1]\n",
    "        axs[1].annotate(f'{int(y)}', (x.mean(), y), ha='center', va='bottom')\n",
    "\n",
    "    # Format x-axes\n",
    "    axs[1].set_xticklabels(axs[1].xaxis.get_majorticklabels(), rotation=90)\n",
    "    axs[0].xaxis.set_visible(False)\n",
    "\n",
    "    # Narrow the gap between the plots\n",
    "    plt.subplots_adjust(hspace=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Oh no, looks like we have run into the problem of overplotting again! \n",
    "\n",
    "You might have noticed that the graph is overplotted because **there are actually quite a few neighborhoods in our dataset**! For the clarity of our visualization, we will have to zoom in again on a few of them. The reason for this is our visualization will become quite cluttered with a super dense x-axis.\n",
    "\n",
    "Assign the variable `in_top_20_neighborhoods` to a copy of `training_data` that has been filtered to only contain rows corresponding to properties that are in one of the top 20 most populous neighborhoods. We define the top 20 neighborhoods as being the 20 neighborhood codes that have the greatest number of properties within them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_20_neighborhood_codes = ...\n",
    "in_top_20_neighborhoods = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create another of the distribution of sale price within in each neighborhood again, but this time with a narrower focus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorical(neighborhoods=in_top_20_neighborhoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 6c\n",
    "\n",
    "From the plot above, we can see that there is much less data available for some neighborhoods. For example, Neighborhood 71 has only around 27% of the number of datapoints as Neighborhood 30.\n",
    "\n",
    "One way we can deal with the lack of data from some neighborhoods is to create a new feature that bins neighborhoods together. We’ll categorize our neighborhoods in a crude way. In this question, we’ll compute how “expensive” each neighborhood is by aggregating the `Log Sale Price`s for all properties in a particular neighborhood using a `metric`, such as the median. We’ll use this `metric` to find the top `n` most expensive neighborhoods. Then, in `q6d`, we’ll label these “expensive neighborhoods” and leave all other neighborhoods unmarked.\n",
    "\n",
    "Fill in `find_expensive_neighborhoods` to return a **list** of the neighborhood codes of the **top `n`** most expensive neighborhoods as measured by our choice of aggregating function, `metric`.\n",
    "\n",
    "For example, calling `find_expensive_neighborhoods(training_data, n=3, metric=np.median)` should return the 3 neighborhood codes with the highest median `Log Sale Price` computed across all properties in those neighborhood codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_expensive_neighborhoods(data, n=3, metric=np.median):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (DataFrame): should contain at least an int-valued 'Neighborhood Code'\n",
    "        and a numeric 'Log Sale Price' column\n",
    "      n (int): the number of top values desired\n",
    "      metric (function): function used for aggregating the data in each neighborhood.\n",
    "        for example, np.median for median prices\n",
    "    \n",
    "    Output:\n",
    "      a list of the the neighborhood codes of the top n highest-priced neighborhoods \n",
    "      as measured by the metric function\n",
    "    \"\"\"\n",
    "    neighborhoods = ...\n",
    "    \n",
    "    # This makes sure the final list contains the generic int type used in Python3, not specific ones used in NumPy.\n",
    "    return [int(code) for code in neighborhoods]\n",
    "\n",
    "expensive_neighborhoods = find_expensive_neighborhoods(training_data, 3, np.median)\n",
    "expensive_neighborhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 6d\n",
    "\n",
    "We now have a list of neighborhoods we've deemed as higher-priced than others.  Let's use that information to write an additional function `add_expensive_neighborhood` that takes in a `DataFrame` of housing data (`data`) and a list of neighborhood codes considered to be expensive (`expensive_neighborhoods`). You can think of `expensive_neighborhoods` as being the output of the function `find_expensive_neighborhoods` from `q6c`. \n",
    "\n",
    "Using these inputs, `add_expensive_neighborhood` should add a column to `data` named `in_expensive_neighborhood` that takes on the **integer** value of 1 if a property is part of a neighborhood in `expensive_neighborhoods` and the integer value of 0 if it is not. This type of variable is known as an **indicator variable**.\n",
    "\n",
    "**Hint:** `pd.Series.astype` [(documentation)](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.Series.astype.html) may be useful for converting `True`/`False` values to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_in_expensive_neighborhood(data, expensive_neighborhoods):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (DataFrame): a DataFrame containing a 'Neighborhood Code' column with values\n",
    "        found in the codebook\n",
    "      expensive_neighborhoods (list of ints): ints should be the neighborhood codes of\n",
    "        neighborhoods pre-identified as expensive\n",
    "    Output:\n",
    "      DataFrame identical to the input with the addition of a binary\n",
    "      in_expensive_neighborhood column\n",
    "    \"\"\"\n",
    "    data['in_expensive_neighborhood'] = ...\n",
    "    return data\n",
    "\n",
    "expensive_neighborhoods = find_expensive_neighborhoods(training_data, 3, np.median)\n",
    "training_data = add_in_expensive_neighborhood(training_data, expensive_neighborhoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following question, we will take a closer look at the `Roof Material` feature of the dataset and examine how we can incorporate categorical features into our linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 7a\n",
    "\n",
    "If we look at `codebook.txt` carefully, we can see that the Assessor's Office uses the following mapping for the numerical values in the `Roof Material` column.\n",
    "```\n",
    "Roof Material (Nominal): \n",
    "\n",
    "       1    Shingle/Asphalt\n",
    "       2    Tar & Gravel\n",
    "       3    Slate\n",
    "       4    Shake\n",
    "       5    Tile\n",
    "       6    Other\n",
    "```\n",
    "\n",
    "Write a function `substitute_roof_material` that replaces each numerical value in `Roof Material` with their corresponding roof material. Your function should return a new `DataFrame`, not modify the existing `DataFrame`. If you modify the existing `DataFrame` by accident, you can load `training_data` again in `q2b`.\n",
    "\n",
    "**Hint**: the `DataFrame.replace` ([documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html)) method may be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def substitute_roof_material(data):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (DataFrame): a DataFrame containing a 'Roof Material' column.  Its values\n",
    "                         should be limited to those found in the codebook\n",
    "    Output:\n",
    "      new DataFrame identical to the input except with a refactored 'Roof Material' column\n",
    "    \"\"\"\n",
    "    ...\n",
    "    return new_data\n",
    "    \n",
    "training_data_mapped = substitute_roof_material(training_data)\n",
    "training_data_mapped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 7b\n",
    "\n",
    "#### An Important Note on One-Hot-Encoding \n",
    "\n",
    "Unfortunately, simply replacing the integers with the appropriate strings isn’t sufficient for using `Roof Material` in our model.  Since `Roof Material` is a categorical variable, we will have to one-hot-encode the data. \n",
    "\n",
    "Complete the following function `ohe_roof_material` that returns a `DataFrame` with the new column one-hot-encoded on the roof material of the household. These new columns should have the form `Roof Material_MATERIAL`. Your function should return a new `DataFrame` and **should not modify the existing `DataFrame`**.\n",
    "\n",
    "You should use `scikit-learn`’s `OneHotEncoder` to perform the one-hot-encoding. `OneHotEncoder` will automatically generate column names of the form `Roof Material_MATERIAL`. Unlike in the lab example however, in this problem we only wish to construct the one-hot-encoding columns **without removing any columns**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def ohe_roof_material(data):\n",
    "    \"\"\"\n",
    "    One-hot-encodes roof material. New columns are of the form \"Roof Material_MATERIAL\".\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "training_data_ohe = ohe_roof_material(training_data_mapped)\n",
    "# This line of code will display only the one-hot-encoded columns in training_data_ohe that \n",
    "# have names that begin with “Roof Material_\" \n",
    "training_data_ohe.filter(regex='^Roof Material_').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 8: Preparing Data\n",
    "\n",
    "\n",
    "Let's split the dataset into a training set and a validation set. We will use the training set to fit our model's parameters and the validation set to evaluate how well our model will perform on unseen data drawn from the same distribution. \n",
    "\n",
    "In the cell below, complete the function `train_val_split` that splits `data` into two smaller `DataFrame`s named `train` and `validation`. Let `train` contain 80% of the data, and let `validation` contain the remaining 20%. **You should not import any additional libraries for this question.** \n",
    "\n",
    "You should only use `NumPy` functions to generate randomness! Your answer should use the variable `shuffled_indices` defined for you. Take a look at the [documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.permutation.html) for `np.permutation` to better understand what `shuffled_indices` contains.\n",
    "\n",
    "**Hint:** While there are multiple solutions, one way is to create two `NumPy` arrays named `train_indices` and `validation_indices` (or any variable names of your choice) that contain a *random* 80% and 20% of the indices, respectively. Then, use these arrays to index into `data` to create your final `train` and `validation` `DataFrame`s. To ensure that your code matches the solution, use the first 80% as the training set and the last 20% as the validation set. Remember, the values you use to partition `data` must be integers!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This makes the train-validation split in this section reproducible across different runs \n",
    "# of the notebook. You do not need this line to run train_val_split in general.\n",
    "\n",
    "# DO NOT CHANGE THIS LINE\n",
    "np.random.seed(1337)\n",
    "# DO NOT CHANGE THIS LINE\n",
    "\n",
    "def train_val_split(data):\n",
    "    \"\"\" \n",
    "    Takes in a DataFrame `data` and randomly splits it into two smaller DataFrames \n",
    "    named `train` and `validation` with 80% and 20% of the data, respectively. \n",
    "    \"\"\"\n",
    "    \n",
    "    data_len = data.shape[0]\n",
    "    shuffled_indices = np.random.permutation(data_len)\n",
    "    ...\n",
    "    train = ...\n",
    "    validation = ...\n",
    "   \n",
    "    return train, validation\n",
    "train, validation = train_val_split(training_val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 9: Fitting a Simple Model\n",
    "\n",
    "Let's fit our linear regression model using the ordinary least squares estimator! We will start with something simple by using only two features: the **number of bedrooms** in the household and the **log-transformed total area covered by the building** (in square feet). \n",
    "\n",
    "Consider the following expression for our first linear model that contains one of the features:\n",
    "\n",
    "$$\n",
    "\\text{Log Sale Price} = \\theta_0 + \\theta_1 \\cdot (\\text{Bedrooms})\n",
    "$$\n",
    "\n",
    "In parallel, we will also consider a second model that contains both features:\n",
    "\n",
    "$$\n",
    "\\text{Log Sale Price} = \\theta_0 + \\theta_1 \\cdot (\\text{Bedrooms}) + \\theta_2 \\cdot (\\text{Log Building Square Feet})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "--- \n",
    "\n",
    "## Question 9a\n",
    "\n",
    "**Without running any calculation or code**, assign `q9a` to be the comparator ('>=', '=', '<=') that fills the blank in the following statement:\n",
    "\n",
    "We quantify the loss on our linear models using MSE (Mean Squared Error). Consider the training loss of the first model and the training loss of the second model. We are guaranteed that:\n",
    "\n",
    "$$\n",
    "\\text{Training Loss of the 1st Model} ~~  \\_\\_\\_\\_ ~~ \\text{Training Loss of the 2nd Model}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q9a = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Pipeline Function\n",
    "\n",
    "You wrote a few functions that added features to the dataset. Instead of calling them manually one by one each time, it is best practice to encapsulate all of this feature engineering into one \"**pipeline**\" function. Defining and using a pipeline reduces all the feature engineering to just one function call and ensures that the same transformations are applied to all data.  Below, we combined some functions into a single helper function that outputs `X` and `Y` for the first model above. Try to understand what this function does! \n",
    "\n",
    "We have provided the \"log_transform\" function for you, for the rest of the functions, use what you have implemented in the previous questions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(data, col):\n",
    "\n",
    "    data['Log '+col] = np.log(data[col])\n",
    "\n",
    "    return data \n",
    "\n",
    "\n",
    "#######################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def feature_engine_simple(data):\n",
    "    # Remove outliers\n",
    "    data = remove_outliers(data, 'Sale Price', lower=500)\n",
    "    # Create Log Sale Price column\n",
    "    data = log_transform(data, 'Sale Price')\n",
    "    # Create Bedroom column\n",
    "    data = add_total_bedrooms(data)\n",
    "    # Select X and Y from the full data\n",
    "    X = data[['Bedrooms']]\n",
    "    Y = data['Log Sale Price']\n",
    "    return X, Y\n",
    "\n",
    "# Reload the data\n",
    "full_data = pd.read_csv(\"cook_county_train.csv\")\n",
    "\n",
    "# Process the data using the pipeline for the first model.\n",
    "np.random.seed(1337)\n",
    "train_m1, valid_m1 = train_val_split(full_data)\n",
    "X_train_m1_simple, Y_train_m1_simple = feature_engine_simple(train_m1)\n",
    "X_valid_m1_simple, Y_valid_m1_simple = feature_engine_simple(valid_m1)\n",
    "\n",
    "# Take a look at the result\n",
    "display(X_train_m1_simple.head())\n",
    "display(Y_train_m1_simple.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.pipe`\n",
    "\n",
    "Alternatively, we can build the pipeline using `pd.DataFrame.pipe` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pipe.html)). Take a look at our use of `pd.DataFrame.pipe` below. \n",
    "\n",
    "The following function `feature_engine_pipe` takes in a `DataFrame` `data`, a list `pipeline_functions` containing 3-element tuples `(function, arguments, keyword_arguments)` that will be called on `data` in the pipeline, and the label `prediction_col` that represents the column of our target variable (`Sale Price` in this case). You can use this function with each of the tuples passed in through `pipeline_functions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to define feature_engine_pipe; no further action is needed.\n",
    "def feature_engine_pipe(data, pipeline_functions, prediction_col):\n",
    "    \"\"\"Process the data for a guided model.\"\"\"\n",
    "    for function, arguments, keyword_arguments in pipeline_functions:\n",
    "        if keyword_arguments and (not arguments):\n",
    "            data = data.pipe(function, **keyword_arguments)\n",
    "        elif (not keyword_arguments) and (arguments):\n",
    "            data = data.pipe(function, *arguments)\n",
    "        else:\n",
    "            data = data.pipe(function)\n",
    "    X = data.drop(columns=[prediction_col])\n",
    "    Y = data.loc[:, prediction_col]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "--- \n",
    "\n",
    "## Question 9b\n",
    "\n",
    "It is time to prepare the training and validation data for the two models we proposed above. Use the following two cells to reload a fresh dataset from scratch and run them through the following preprocessing steps using `feature_engine_pipe` for each model:\n",
    "\n",
    "- Perform a `train_val_split` on the original dataset, loaded as the `DataFrame` `full_data`. Let 80% of the set be training data, and 20% of the set be validation data. \n",
    "- For both the training and validation set,\n",
    "    1. Remove outliers in `Sale Price` so that we consider households with a price that is greater than 499 dollars (or equivalently, a price that is 500 dollars or greater). \n",
    "    2. Apply log transformations to the `Sale Price` and the `Building Square Feet` columns to create two new columns, `Log Sale Price` and `Log Building Square Feet`.\n",
    "    3. Extract the total number of bedrooms into a new column `Bedrooms` from the `Description` column.\n",
    "    4. Select the columns `Log Sale Price` and `Bedrooms` (and `Log Building Square Feet` if this is the second model). We have implemented the helper function `select_columns` for you.\n",
    "    5. Return the design matrix $\\mathbb{X}$ and the observed vector $\\mathbb{Y}$. Note that $\\mathbb{Y}$ refers to the transformed `Log Sale Price`, not the original `Sale Price`. **Your design matrix and observed vector should be `NumPy` arrays or `pandas` `DataFrame`s**.\n",
    "\n",
    "Assign the final training data and validation data for both models to the following set of variables:\n",
    "\n",
    "- First Model: `X_train_m1`, `Y_train_m1`, `X_valid_m1`, `Y_valid_m1`. This is already implemented for you. \n",
    "- Second Model: `X_train_m2`, `Y_train_m2`, `X_valid_m2`, `Y_valid_m2`. Please implement this in the second cell below. You may use the first model as an example.\n",
    "\n",
    "For an example of how to work with pipelines, we have processed model 1 for you using `m1_pipelines` by passing in the corresponding pipeline functions as a list of tuples in the below cell. Your task is to do the same for model 2 in the cell after —— that is, save your pipeline functions as a list of tuples and assign it to `m2_pipelines` for model 2.\n",
    "\n",
    "As a refresher, the equations model 1 and model 2, respectively, are:\n",
    "$$\n",
    "\\text{Log Sale Price} = \\theta_0 + \\theta_1 \\cdot (\\text{Bedrooms})\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Log Sale Price} = \\theta_0 + \\theta_1 \\cdot (\\text{Bedrooms}) + \\theta_2 \\cdot (\\text{Log Building Square Feet})\n",
    "$$\n",
    "\n",
    "**Note**: Do not change the line `np.random.seed(1337)` as it ensures we are partitioning the dataset the same way for both models (otherwise, their performance isn't directly comparable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data\n",
    "full_data = pd.read_csv(\"cook_county_train.csv\")\n",
    "\n",
    "# Apply feature engineering to the data using the pipeline for the first model\n",
    "np.random.seed(1337)\n",
    "train_m1, valid_m1 = train_val_split(full_data)\n",
    "\n",
    "# Helper function\n",
    "def select_columns(data, *columns):\n",
    "    \"\"\"Select only columns passed as arguments.\"\"\"\n",
    "    return data.loc[:, columns]\n",
    "\n",
    "# Pipelines, a list of tuples\n",
    "m1_pipelines = [\n",
    "    (remove_outliers, None, {\n",
    "        'variable': 'Sale Price',\n",
    "        'lower': 500,\n",
    "    }),\n",
    "    (log_transform, None, {'col': 'Sale Price'}),\n",
    "    (add_total_bedrooms, None, None),\n",
    "    (select_columns, ['Log Sale Price', 'Bedrooms'], None)\n",
    "]\n",
    "\n",
    "X_train_m1, Y_train_m1 = feature_engine_pipe(train_m1, m1_pipelines, 'Log Sale Price')\n",
    "X_valid_m1, Y_valid_m1 = feature_engine_pipe(valid_m1, m1_pipelines, 'Log Sale Price')\n",
    "\n",
    "# Take a look at the result\n",
    "# It should be the same above as the result returned by feature_engine_simple\n",
    "display(X_train_m1.head())\n",
    "display(Y_train_m1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS LINE\n",
    "np.random.seed(1337)\n",
    "# DO NOT CHANGE THIS LINE\n",
    "\n",
    "# Process the data using the pipeline for the second model\n",
    "train_m2, valid_m2 = ...\n",
    "\n",
    "m2_pipelines = ...\n",
    "\n",
    "X_train_m2, Y_train_m2 = ...\n",
    "X_valid_m2, Y_valid_m2 = ...\n",
    "\n",
    "\n",
    "# Take a look at the result\n",
    "display(X_train_m2.head())\n",
    "display(Y_train_m2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "--- \n",
    "\n",
    "## Question 9c\n",
    "\n",
    "Finally, let's do some regression!\n",
    "\n",
    "We first initialize a `sklearn.linear_model.LinearRegression` object [(documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) for both of our models. We set the `fit_intercept = True` to ensure that the linear model has a non-zero intercept (i.e., a bias term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_m1 = lm.LinearRegression(fit_intercept=True)\n",
    "linear_model_m2 = lm.LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to fit our linear regression model. Use the cell below to fit both models and then use it to compute the fitted values of `Log Sale Price` over the training data and the predicted values of `Log Sale Price` for the validation data.\n",
    "\n",
    "Assign the predicted values from both of your models on the training and validation set to the following variables:\n",
    "\n",
    "- First Model: predicted values on **training set**: `Y_fitted_m1`, predicted values on **validation set**: `Y_predicted_m1`\n",
    "- Second Model: predicted values on **training set**: `Y_fitted_m2`, predicted values on **validation set**: `Y_predicted_m2`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the 1st model\n",
    "...\n",
    "# Compute the fitted and predicted values of Log Sale Price for 1st model\n",
    "Y_fitted_m1 = ...\n",
    "Y_predicted_m1 = ...\n",
    "\n",
    "# Fit the 2nd model\n",
    "...\n",
    "# Compute the fitted and predicted values of Log Sale Price for 2nd model\n",
    "Y_fitted_m2 = ...\n",
    "Y_predicted_m2 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 10: Evaluate Our Simple Model\n",
    "\n",
    "<br>\n",
    "\n",
    "--- \n",
    "\n",
    "Let's now move into the analysis of our two models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predicted, actual):\n",
    "    \"\"\"\n",
    "    Calculates RMSE from actual and predicted values.\n",
    "    Input:\n",
    "      predicted (1D array): Vector of predicted/fitted values\n",
    "      actual (1D array): Vector of actual values\n",
    "    Output:\n",
    "      A float, the RMSE value.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((actual - predicted)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "--- \n",
    "\n",
    "## Question 10a\n",
    "\n",
    "One way of understanding a model's performance (and appropriateness) is through a plot of the residuals versus the observations.\n",
    "\n",
    "In the cell below, use `plt.scatter` [(documentation)](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html) to plot the residuals from predicting `Log Sale Price` using **only the second model** against the original `Log Sale Price` for the **validation data**. With such a large dataset, it is difficult to avoid overplotting entirely. You should also **ensure that the dot size and opacity in the scatter plot are set appropriately** to reduce the impact of overplotting as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our simple model explains some of the variability in price, there is certainly still a lot of room for improvement —— one reason is we have been only utilizing 1 or 2 features (out of a total of 70+) so far! Can you engineer and incorporate more features to improve the model's fairness and accuracy? We won't be asking you to provide your answers here, but this will be important going into the next part of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 11\n",
    "\n",
    "It is time to build your own model!\n",
    "\n",
    "You will conduct feature engineering on your training data using the `feature_engine_final` function, fit the model with this training data, and compute the training Root Mean Squared Error (RMSE). Then, we will process our test data with `feature_engine_final`, use the model to predict `Log Sale Price` for the test data, transform the predicted and original log values back into their original forms (by using `delog`), and compute the test RMSE.\n",
    "\n",
    "Note your `feature_engine_final` can contain any function you have created before and any new function that you wish to explore. \n",
    "\n",
    "Your goal in Question 11 is to:\n",
    "\n",
    "* Define a function to perform feature engineering and produce a design matrix for modeling.\n",
    "* Apply this feature engineering function to the training data and use it to train a model that can predict the `Log Sale Price` of houses.\n",
    "* Use this trained model to predict the `Log Sale Price`s of the test set. Remember that our test set does not contain the true `Sale Price` of each house –— your model is trying to guess them! \n",
    "* Submit your predicted `Log Sale Price`s on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Scheme\n",
    "\n",
    "Your grade for the project will be based on your test RMSE and your ipynb. The breakdown are as follows:\n",
    "\n",
    "1. Completeness of your ipynb(20 pts)\n",
    "\n",
    "2. \n",
    "Points | 30 | 25 | 20 | 15\n",
    "--- | --- | --- | --- | ---\n",
    "Test RMSE | Top 20% | (20%, 40%] | (40%, 70%] | Last 30%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your analysis goes to this part \n",
    "\n",
    "training_val_data = pd.read_csv(\"cook_county_train.csv\", index_col='Unnamed: 0')\n",
    "\n",
    "test_data = pd.read_csv(\"cook_county_test.csv\")\n",
    "\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For submission\n",
    "\n",
    "\n",
    "y_test_predicted = model.predict(X_test)\n",
    "### Keep log Sale Price in your csv file\n",
    "\n",
    "predictions = pd.DataFrame({'Log Sale Price': y_test_predicted})\n",
    "predictions.to_csv('predictions.csv')\n",
    "print('Your predictions have been exported as predictions.csv. Please download the file and submit it to Canvas. ')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "otter": {
   "OK_FORMAT": true,
   "require_no_pdf_confirmation": true,
   "tests": {
    "q2b": {
     "name": "q2b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 'Sale Price' in training_data.columns\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 'Log Sale Price' in training_data.columns\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> training_data.shape == (168931, 63)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(training_data['Log Sale Price'].sum(), 2055590.7351105125, atol=1e-05)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> q3statement in [True, False]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 'Building Square Feet' in training_data.columns\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(training_data['Building Square Feet'].mean(), 1626.422290757765)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 'Log Building Square Feet' in training_data.columns\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(training_data['Log Building Square Feet'].mean(), 7.2914293261161)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> remove_outliers(training_data, 'Building Square Feet', upper=2000).shape != training_data.shape\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5a": {
     "name": "q5a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.all([num in np.arange(1, 9) for num in q5a])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(q5a, list) == True\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(q5a[0], int) == True\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5b": {
     "name": "q5b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> not training_data['Bedrooms'].isnull().any()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> pd.api.types.is_integer_dtype(training_data['Bedrooms'])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6a": {
     "name": "q6a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(num_neighborhoods, int)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6b": {
     "name": "q6b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(in_top_20_neighborhoods['Neighborhood Code'].unique()) == 20\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> in_top_20_neighborhoods['Neighborhood Code'].iloc[0] == 120\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6c": {
     "name": "q6c",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(find_expensive_neighborhoods(training_data, 5, np.median)) == 5\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(expensive_neighborhoods, list)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(find_expensive_neighborhoods(training_data, 2, np.min)) == set([106, 580])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6d": {
     "name": "q6d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> sum(training_data.loc[:, 'in_expensive_neighborhood']) == 1290 and sum(training_data.loc[:, 'in_expensive_neighborhood'].isnull()) == 0\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7a": {
     "name": "q7a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> set(training_data_mapped['Roof Material'].unique()) == set(['Shingle/Asphalt', 'Tar & Gravel', 'Other', 'Tile', 'Shake', 'Slate'])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> training_data.shape == training_data_mapped.shape\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> training_data['Roof Material'].dtype == np.float64\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7b": {
     "name": "q7b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> training_data_ohe.shape[1] - training_data.shape[1] == 6\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> expected_ohe_cols = set(['Roof Material_Other', 'Roof Material_Shake', 'Roof Material_Shingle/Asphalt', 'Roof Material_Slate', 'Roof Material_Tar & Gravel', 'Roof Material_Tile'])\n>>> expected_ohe_cols.issubset(set(training_data_ohe.columns)) == True\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "qsurvey": {
     "name": "qsurvey",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(qsurvey, str)\n>>> assert ' ' not in qsurvey\n>>> assert len(qsurvey) == 12\n>>> assert qsurvey[0] == 'P' and qsurvey[-1] == 'R'\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
